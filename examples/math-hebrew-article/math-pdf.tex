\documentclass{article}
\usepackage{parskip}
\usepackage[english,hebrew]{babel}
\usepackage{amsmath, amssymb}
%\usepackage{lmodern}

\newcommand{\LR}{\L}
\newcommand{\RL}{\R}
\newcommand{\EN}{\selectlanguage{english}}
\newcommand{\HE}{\selectlanguage{hebrew}}
\usepackage{array}
\newcolumntype{H}{>{\RL\bgroup}r<{\egroup}}

\title{ייצוג משוואות ריגרסיה ע״י וקטורים ומטריצות}
\author{}
\date{}
\begin{document}
	\section{ריגרסיה --- מבוא}
	בסטטיסטיקה, ניתוח רגרסיה הוא שם כולל למשפחה של מודלים סטטיסטיים להערכת הקשרים בין משתנים. המשותף לכל המודלים הוא קיומם של משתנה מוסבר (המכונה לעיתים בשם המשתנה התלוי) ומשתנה מסביר אחד או יותר (המכונים לעיתים בשם המשתנים הבלתי תלויים או המשתנים המנבאים). בעזרת מודל רגרסיה ניתן ללמוד כיצד ערכו של המשתנה המוסבר משתנה כאשר חל שינוי בערכו של אחד המשתנים המסבירים, וערכי שאר המשתנים המסבירים נשארים קבועים. עם זאת, אין בכך די כדי להסיק סיבתיות: השינוי בערכו של המשתנה המסביר לא בהכרח גורם לשינוי בערכו של המשתנה המוסבר.

\subsection{מודל מתמטי}
נניח מודל:
\[y_i = \beta_0 + \beta_1 x_{1_i} + \dots + \beta_k x_k + u\]
ונניח כ-$n$ תצפיות. אזי:
$$
\begin{pmatrix}
	y_1 \\
	y_2 \\
	\vdots \\
	y_n
\end{pmatrix} = \begin{pmatrix}
1 & x_{1,1} & x_{2,1} & \dots & x_{k,1} \\
1 & x_{1,2} & x_{2,2} & \dots & x_{k,2} \\
\vdots & \dots \\
1 & x_{1,n} & {x_2,n} & \dots & x_{k,n}
\end{pmatrix} \begin{pmatrix}
\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k
\end{pmatrix} + \begin{pmatrix}
u_1 \\ u_2 \\ \vdots \\ u_n
\end{pmatrix}
$$

\subsection{אומד הריבועים הפחותים \LR{(OLS)}}
רוצים להביא למינימום את סכום השאריות. כלומר:
\[
\min_{\hat{\beta_0}, \hat{\beta_1}, \dots \hat{\beta_k}} \sum_{i=1}^n (y_i - \hat{y_i})^2
\]

\subsection{טבלה}
\begin{tabular}{|H|H|H|}
	\RL{תא אחרון} & \RL{תא אמצעי} & \RL{תא ראשון} \\
\hline
	תא ראשון & תא אמצעי & תא אחרון
	
\end{tabular}
\begin{itemize}
	\item פריט ראשון
	\item פריט שני
\end{itemize}


\end{document}